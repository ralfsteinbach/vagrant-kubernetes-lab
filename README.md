# vagrant-kubernetes-lab
This project is to provide a simple local Kubernetes cluster with a master and a worker node for testing purpose.

Vagrant was chosen for being able to create the cluster either on linux, windows and mac hosts.

![Vagrant + Docker + Kubernetes](https://cdn-images-1.medium.com/max/1600/1*qfRHBkxKEwK8nfnwGe6WCQ.png)

## Prequirements
- Vagrant : https://www.vagrantup.com/downloads.html
- VirtualBox : https://www.virtualbox.org/wiki/Downloads

## Default provisioning
Vagrant up will provide 2 Ubuntu Xenial VMs for a kubernetes cluster with these features by default :
- Kubernetes control plane (latest)
- Weave network cni (latest)
- Kubernetes Dashboard (latest)
- Helm (latest)

## Options

### network
By default, we will use the Weave-Net driver but you can override it with flannel like this:
```
 vagrant --network=flannel up
```

### bridged-network
By default, we will use the Virtual Box ```private_network``` but you can use a ```briged_network``` by giving the name of the network adapter like this:
```
 vagrant --bridged-network=<network adapter name> up
```
Depending on this setting, it may be necessary to adapt the ip addresses of the nodes (boxes section of Vagrantfile) to fall into the iprange of your network.

### mount
It is possible to mount a directory from the host to the /data directory on each node.  This option could be used, for example, to make git repositories local to the host available to be used in the cluster.

The `--mount=<pathOnHost>` option allows to choose the directory on the host that will be mounted to `/data`.  If that option is not specified, nothing will be mounted.  
For example, run the command:
```
 vagrant --mount=/home/user/git up
```

### dockerhub login
You can provide your username and password in order to do a "docker login" on each node of the cluster.
This will allow you to pull private images from the Docker registry.
```
 vagrant --docker-username=myUser --docker-password=myPassword up
```

## Kubeconfig
After installation, a copy of the admin.conf generated by kubeadm is available on the host in the /vagrant/kubeconfig directory, which is a shared folder inside the Git repo ./kubeconfig

Even if kubectl can be used within the guests, it is suggested that you copy the admin.conf as config in your ~/.kube/ directory on the host to able to do kubectl proxy and kubectl port-forward per example.

Or you can simply execute:
```
export KUBECONFIG=`pwd`/kubeconfig/admin.conf
kubectl get nodes
```
And you should see:
```
NAME        STATUS    ROLES     AGE       VERSION
k8smaster   Ready     master    10m       v1.9.0
k8sworker   Ready     <none>    5m        v1.9.0
```

## Extra features 
Multiple optionnal extra softwares has been tested with this cluster and are available if needed.

Here is the current list by category :
### Ingress
- Nginx ingress controller v0.9.0-beta.15 + Default backend v1.4
### Monitoring
- InfluxDB v1.3.3 + Grafana v4.4.3
- Weavescope v1.6.7
### Networking
- Weave net (latest)
- Flannel v0.9.1
### Storage
- Nfs-provisioner v1.0.9
### Helm
- Helm (latest)
Installed by default, only helm client is required to fit your host OS : https://github.com/kubernetes/helm/releases
##### Have a look at the README.md in related subdirectories for specific instructions.

## Install everything (Quick start)

Note that if you want to install everything, you can simply run the following script (providing you have kubectl installed):

```
vagrant up
bash scripts/install-all-features.sh
kubectl get pods --all-namespaces -o wide
```

And repeat the get "kubectl get pods" command as long as the pods are not up and running.

## Open dashboards

### Kubernetes

Follow the instructions printed in the console:
```
bash scripts/open-dashboard.sh
```

### Grafana

Note that you might have to reload the page in your browser.

```
bash scripts/open-grafana.sh
```

### Weave Scope

Note that you might have to reload the page in your browser.

```
bash scripts/open-weavescope.sh
```
